#!/bin/bash

# Kolory dla lepszej czytelnoÅ›ci
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}ðŸš€ OLLAMA SERVER STARTUP SCRIPT${NC}"
echo -e "${BLUE}================================${NC}"

# Funkcja logowania
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Sprawdzenie architektury systemu
ARCH=$(uname -m)
OS=$(uname -s)
log_info "Wykryto system: ${OS} ${ARCH}"

# DostÄ™pne modele 2B (optymalne dla sÅ‚abszego sprzÄ™tu)
MODELS_2B=(
    "llama3.2:1b"      # Najszybszy - 1B parametrÃ³w
    "gemma2:2b"        # Dobry balans jakoÅ›Ä‡/szybkoÅ›Ä‡ - 2B parametrÃ³w
    "qwen2.5:1.5b"     # Åšwietny do jÄ™zykÃ³w azjatyckich - 1.5B
    "phi3.5"           # Microsoft model - ~3.8B ale zoptymalizowany
    "tinyllama"        # Ultra szybki - 1.1B parametrÃ³w
)

# Konfiguracja
OLLAMA_HOST=${OLLAMA_HOST:-"localhost"}
OLLAMA_PORT=${OLLAMA_PORT:-"11434"}
SELECTED_MODEL=${OLLAMA_MODEL:-"gemma2:2b"}
OLLAMA_URL="http://${OLLAMA_HOST}:${OLLAMA_PORT}"

# Funkcja sprawdzenia czy Ollama jest zainstalowana
check_ollama_installed() {
    if command -v ollama &> /dev/null; then
        log_info "Ollama jest zainstalowana: $(ollama --version 2>/dev/null || echo 'unknown version')"
        return 0
    else
        log_warn "Ollama nie jest zainstalowana"
        return 1
    fi
}

# Funkcja instalacji Ollama
install_ollama() {
    log_info "Instalacja Ollama..."

    case "$OS" in
        "Linux")
            # Automatyczna instalacja dla Linux
            curl -fsSL https://ollama.ai/install.sh | sh
            ;;
        "Darwin")
            # macOS
            if command -v brew &> /dev/null; then
                brew install ollama
            else
                log_error "Na macOS zainstaluj brew lub pobierz Ollama z https://ollama.ai"
                exit 1
            fi
            ;;
        *)
            log_error "NieobsÅ‚ugiwany system operacyjny: $OS"
            log_info "Pobierz Ollama z https://ollama.ai"
            exit 1
            ;;
    esac

    # Sprawdzenie czy instalacja siÄ™ powiodÅ‚a
    if check_ollama_installed; then
        log_info "âœ… Ollama zainstalowana pomyÅ›lnie"
    else
        log_error "âŒ Instalacja Ollama nie powiodÅ‚a siÄ™"
        exit 1
    fi
}

# Funkcja sprawdzenia czy Ollama server dziaÅ‚a
check_ollama_running() {
    if curl -s "${OLLAMA_URL}/api/tags" > /dev/null 2>&1; then
        return 0
    else
        return 1
    fi
}

# Funkcja uruchamiania Ollama server
start_ollama_server() {
    log_info "Uruchamianie Ollama server..."

    if check_ollama_running; then
        log_info "âœ… Ollama server juÅ¼ dziaÅ‚a na ${OLLAMA_URL}"
        return 0
    fi

    # Uruchomienie w tle
    case "$OS" in
        "Linux")
            # SprawdÅº czy systemd service istnieje
            if systemctl list-unit-files | grep -q ollama.service; then
                log_info "Uruchamianie przez systemctl..."
                sudo systemctl start ollama
                sudo systemctl enable ollama
            else
                log_info "Uruchamianie w tle..."
                nohup ollama serve > ollama.log 2>&1 &
                echo $! > ollama.pid
            fi
            ;;
        "Darwin")
            # macOS - uruchomienie w tle
            log_info "Uruchamianie w tle na macOS..."
            nohup ollama serve > ollama.log 2>&1 &
            echo $! > ollama.pid
            ;;
    esac

    # Oczekiwanie na uruchomienie
    log_info "Oczekiwanie na uruchomienie serwera..."
    for i in {1..30}; do
        if check_ollama_running; then
            log_info "âœ… Ollama server uruchomiony pomyÅ›lnie!"
            return 0
        fi
        sleep 2
        echo -n "."
    done

    log_error "âŒ Nie udaÅ‚o siÄ™ uruchomiÄ‡ Ollama server"
    return 1
}

# Funkcja pobierania modelu
pull_model() {
    local model=$1
    log_info "Pobieranie modelu: ${model}"

    # SprawdÅº czy model juÅ¼ istnieje
    if ollama list | grep -q "$model"; then
        log_info "âœ… Model ${model} juÅ¼ istnieje"
        return 0
    fi

    # Pobierz model
    log_info "â¬‡ï¸ Pobieranie ${model}... (moÅ¼e potrwaÄ‡ kilka minut)"
    if ollama pull "$model"; then
        log_info "âœ… Model ${model} pobrany pomyÅ›lnie"
        return 0
    else
        log_error "âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ modelu ${model}"
        return 1
    fi
}

# Funkcja testowania modelu
test_model() {
    local model=$1
    log_info "Testowanie modelu: ${model}"

    local test_prompt="Napisz krÃ³tkÄ… odpowiedÅº na email: 'DzieÅ„ dobry, mam pytanie o PaÅ„stwa produkt.'"

    log_info "WysyÅ‚anie test promptu..."
    local response=$(curl -s -X POST "${OLLAMA_URL}/api/generate" \
        -d "{\"model\":\"${model}\",\"prompt\":\"${test_prompt}\",\"stream\":false}" \
        | grep -o '"response":"[^"]*"' | sed 's/"response":"//;s/"$//')

    if [ -n "$response" ] && [ "$response" != "null" ]; then
        log_info "âœ… Model dziaÅ‚a! PrzykÅ‚adowa odpowiedÅº:"
        echo -e "${BLUE}${response}${NC}"
        return 0
    else
        log_error "âŒ Model nie odpowiada poprawnie"
        return 1
    fi
}

# Funkcja wyÅ›wietlania menu wyboru modelu
select_model() {
    echo -e "\n${YELLOW}DostÄ™pne modele 2B (optymalne dla sÅ‚abszego sprzÄ™tu):${NC}"
    echo "1) llama3.2:1b     - Najszybszy, 1B parametrÃ³w (~1.3GB RAM)"
    echo "2) gemma2:2b       - Dobry balans, 2B parametrÃ³w (~2.6GB RAM) [DOMYÅšLNY]"
    echo "3) qwen2.5:1.5b    - WielojÄ™zyczny, 1.5B parametrÃ³w (~2GB RAM)"
    echo "4) phi3.5          - Microsoft, ~3.8B parametrÃ³w (~4GB RAM)"
    echo "5) tinyllama       - Ultra szybki, 1.1B parametrÃ³w (~1.4GB RAM)"
    echo "6) WÅ‚asny model"
    echo ""

    read -p "Wybierz model (1-6) [domyÅ›lnie 2]: " choice

    case $choice in
        1) SELECTED_MODEL="llama3.2:1b" ;;
        3) SELECTED_MODEL="qwen2.5:1.5b" ;;
        4) SELECTED_MODEL="phi3.5" ;;
        5) SELECTED_MODEL="tinyllama" ;;
        6)
            read -p "Podaj nazwÄ™ modelu: " custom_model
            SELECTED_MODEL="$custom_model"
            ;;
        *) SELECTED_MODEL="gemma2:2b" ;;
    esac

    log_info "Wybrany model: ${SELECTED_MODEL}"
}

# Funkcja gÅ‚Ã³wna
main() {
    echo -e "${BLUE}Parametry:${NC}"
    echo "- Host: ${OLLAMA_HOST}"
    echo "- Port: ${OLLAMA_PORT}"
    echo "- Model: ${SELECTED_MODEL}"
    echo ""

    # SprawdÅº argumenty
    case "${1:-}" in
        "install")
            install_ollama
            exit $?
            ;;
        "stop")
            log_info "Zatrzymywanie Ollama server..."
            if [ -f ollama.pid ]; then
                kill $(cat ollama.pid) 2>/dev/null
                rm ollama.pid
            fi
            sudo systemctl stop ollama 2>/dev/null || true
            log_info "âœ… Ollama zatrzymana"
            exit 0
            ;;
        "status")
            if check_ollama_running; then
                log_info "âœ… Ollama server dziaÅ‚a na ${OLLAMA_URL}"
                echo -e "\n${YELLOW}DostÄ™pne modele:${NC}"
                ollama list
            else
                log_warn "âŒ Ollama server nie dziaÅ‚a"
            fi
            exit 0
            ;;
        "select")
            select_model
            ;;
        "test")
            if [ -n "${2:-}" ]; then
                test_model "$2"
            else
                test_model "$SELECTED_MODEL"
            fi
            exit $?
            ;;
    esac

    # GÅ‚Ã³wny flow
    log_info "Rozpoczynanie konfiguracji Ollama..."

    # 1. SprawdÅº/zainstaluj Ollama
    if ! check_ollama_installed; then
        read -p "Czy zainstalowaÄ‡ Ollama? (y/N): " install_choice
        if [[ $install_choice =~ ^[Yy]$ ]]; then
            install_ollama
        else
            log_error "Ollama jest wymagana do dziaÅ‚ania"
            exit 1
        fi
    fi

    # 2. Uruchom server
    if ! start_ollama_server; then
        log_error "Nie udaÅ‚o siÄ™ uruchomiÄ‡ Ollama server"
        exit 1
    fi

    # 3. Wybierz model (jeÅ›li nie podano argumentu)
    if [[ "${1:-}" == "select" ]] || [[ -z "${OLLAMA_MODEL:-}" ]]; then
        select_model
    fi

    # 4. Pobierz model
    if ! pull_model "$SELECTED_MODEL"; then
        log_error "Nie udaÅ‚o siÄ™ pobraÄ‡ modelu"
        exit 1
    fi

    # 5. Testuj model
    if ! test_model "$SELECTED_MODEL"; then
        log_warn "Model moÅ¼e mieÄ‡ problemy, ale kontynuujemy..."
    fi

    # 6. Podsumowanie
    echo -e "\n${GREEN}ðŸŽ‰ OLLAMA SERVER GOTOWY!${NC}"
    echo -e "${GREEN}================================${NC}"
    echo "ðŸ”— URL: ${OLLAMA_URL}"
    echo "ðŸ¤– Model: ${SELECTED_MODEL}"
    echo "ðŸ“Š Status: ollama list"
    echo "ðŸ”¥ Test: curl -X POST ${OLLAMA_URL}/api/generate -d '{\"model\":\"${SELECTED_MODEL}\",\"prompt\":\"Hello\",\"stream\":false}'"
    echo ""
    echo -e "${YELLOW}DostÄ™pne komendy:${NC}"
    echo "./start-ollama-2b.sh status   - sprawdÅº status"
    echo "./start-ollama-2b.sh stop     - zatrzymaj server"
    echo "./start-ollama-2b.sh select   - wybierz inny model"
    echo "./start-ollama-2b.sh test     - przetestuj model"
    echo ""

    # Zapisz konfiguracjÄ™ do .env
    if [ ! -f .env.ollama ]; then
        cat > .env.ollama << EOF
# Ollama configuration
OLLAMA_HOST=${OLLAMA_HOST}
OLLAMA_PORT=${OLLAMA_PORT}
OLLAMA_MODEL=${SELECTED_MODEL}
OLLAMA_URL=${OLLAMA_URL}
EOF
        log_info "ðŸ’¾ Konfiguracja zapisana do .env.ollama"
    fi
}

# SprawdÅº czy script jest uruchamiany bezpoÅ›rednio
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi